{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building AI Applications with ChatGPT\n",
    "\n",
    "Sumudu Tennakoon, PhD\n",
    "<hr>\n",
    "\n",
    "# Getting Started With ChatGPT API\n",
    "\n",
    "In this notebook we will explore some basic fetures on Python programing language for those who have a prior programing expereince.\n",
    "\n",
    "To learn more about Python, refeer to the following websites\n",
    "\n",
    "- Python : https://www.python.org\n",
    "\n",
    "To learn more about the Python packages we explore in this notebook, refer to the following websites\n",
    "\n",
    "- OpenAI API : https://platform.openai.com/docs/api-reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Library Installation\n",
    "\n",
    "* Run below code cell to install required libraries before you continue. Ignore that if you already installed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer/Encoder \n",
    "### Text to Numeric Representation of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\"You can think of tokens as pieces of words, where 1,000 tokens is about 750 words.\"```\n",
    "\n",
    "https://openai.com/pricing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tiktoken`  BPE (Byte pair encoding) tokeniser for use with OpenAI's models.\n",
    "* https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12331,\n",
       " 648,\n",
       " 13182,\n",
       " 648,\n",
       " 574,\n",
       " 264,\n",
       " 83323,\n",
       " 323,\n",
       " 8590,\n",
       " 380,\n",
       " 889,\n",
       " 13375,\n",
       " 71674,\n",
       " 3495,\n",
       " 389,\n",
       " 9063,\n",
       " 7323,\n",
       " 323,\n",
       " 2834,\n",
       " 1403,\n",
       " 48078,\n",
       " 2394,\n",
       " 4861,\n",
       " 13]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "text = \"Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.\"\n",
    "\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Tokens\n",
    "num_tokens = # Type your code here \n",
    "\n",
    "num_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full Text\n",
    "\n",
    "decoded_tokens_text = encoding.decode(tokens)\n",
    "\n",
    "decoded_tokens_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Mar',\n",
       " b'ie',\n",
       " b' Cur',\n",
       " b'ie',\n",
       " b' was',\n",
       " b' a',\n",
       " b' physicist',\n",
       " b' and',\n",
       " b' chem',\n",
       " b'ist',\n",
       " b' who',\n",
       " b' conducted',\n",
       " b' pioneering',\n",
       " b' research',\n",
       " b' on',\n",
       " b' radio',\n",
       " b'activity',\n",
       " b' and',\n",
       " b' won',\n",
       " b' two',\n",
       " b' Nobel',\n",
       " b' Pr',\n",
       " b'izes',\n",
       " b'.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By token\n",
    "\n",
    "decoded_tokens_list = [encoding.decode_single_token_bytes(token) for token in tokens]\n",
    "\n",
    "decoded_tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion\n",
    "* Need OpenAI API Key \n",
    "  - Adds a cost per 1000 tokens after free trial period (https://openai.com/pricing)\n",
    "  - GPT-3.5 Turbo (4K context): \tInput =\t$0.0015 / 1K tokens, Output =\t$0.002 / 1K tokens (2023-07-09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(r'../../../config.ini') #Change to your path or assign API Key to openai_api_key (not recomended for production)\n",
    "\n",
    "openai_api_key = config['SECRETS']['openai_api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8WGIifgFRNp8KhNw3J3HJEQ8ohRO9', choices=[Choice(finish_reason='length', index=0, message=ChatCompletionMessage(content='In Python, you can use the `len()` function to get the length of a string. Here\\'s an example:\\n\\n```python\\nstring = \"Hello, World!\"\\nlength = len(string)\\nprint(length)\\n```\\n\\nOutput:\\n```\\n13\\n```\\n\\nIn this example, the `len()` function is used to get the length of the string \"Hello, World!\" and store it in the variable `length`. The `print()` function is then used to display the length of the string.', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1702699848, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=53, total_tokens=153))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful learning assitant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"can you help me with python coding problem?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I’d be happy to help!\"},\n",
    "        {\"role\": \"user\", \"content\": \"How can I get length of a string?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-8WGIifgFRNp8KhNw3J3HJEQ8ohRO9',\n",
       " 'choices': [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'message': {'content': 'In Python, you can use the `len()` function to get the length of a string. Here\\'s an example:\\n\\n```python\\nstring = \"Hello, World!\"\\nlength = len(string)\\nprint(length)\\n```\\n\\nOutput:\\n```\\n13\\n```\\n\\nIn this example, the `len()` function is used to get the length of the string \"Hello, World!\" and store it in the variable `length`. The `print()` function is then used to display the length of the string.',\n",
       "    'role': 'assistant',\n",
       "    'function_call': None,\n",
       "    'tool_calls': None},\n",
       "   'logprobs': None}],\n",
       " 'created': 1702699848,\n",
       " 'model': 'gpt-3.5-turbo-0613',\n",
       " 'object': 'chat.completion',\n",
       " 'system_fingerprint': None,\n",
       " 'usage': {'completion_tokens': 100, 'prompt_tokens': 53, 'total_tokens': 153}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response as Dictionary\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Response - Message Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Python, you can use the `len()` function to get the length of a string. Here\\'s an example:\\n\\n```python\\nstring = \"Hello, World!\"\\nlength = len(string)\\nprint(length)\\n```\\n\\nOutput:\\n```\\n13\\n```\\n\\nIn this example, the `len()` function is used to get the length of the string \"Hello, World!\" and store it in the variable `length`. The `print()` function is then used to display the length of the string.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Python, you can use the `len()` function to get the length of a string. Here's an example:\n",
      "\n",
      "```python\n",
      "string = \"Hello, World!\"\n",
      "length = len(string)\n",
      "print(length)\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "13\n",
      "```\n",
      "\n",
      "In this example, the `len()` function is used to get the length of the string \"Hello, World!\" and store it in the variable `length`. The `print()` function is then used to display the length of the string.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=100, prompt_tokens=53, total_tokens=153)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage.total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pricing: https://openai.com/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completion Cost = $0.000253\n"
     ]
    }
   ],
   "source": [
    "# Cost Calculator for GPT-3.5 Turbo\n",
    "input_token_cost = 0.0010/1000\n",
    "output_token_cost = 0.0020/1000\n",
    "\n",
    "cost = response.usage.prompt_tokens*input_token_cost + response.usage.completion_tokens*output_token_cost\n",
    "\n",
    "print(F\"Chat Completion Cost = ${cost:.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print Reponse with proper formatting\n",
    "# Type your code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Add another chat request asking to modify the script to \n",
    "# return \"large string\" if length > 10 else \"small string\"\n",
    "\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response =  # COntinue typing your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-conversation-based Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8WGJ0EF6aHYAhKb9o42etmpT1Z3p1', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Sure! Gravity is a force that pulls things towards each other. It's what keeps us on the ground and makes things fall down. You know how when you drop something, like a ball, it falls to the ground? That's because of gravity. Gravity is also what keeps the planets in our solar system orbiting around the sun. It's a really important force that affects everything in the universe!\", role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1702699866, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=81, prompt_tokens=27, total_tokens=108))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are helpful learning assitant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain gravity as a fifth grader?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Gravity is a force that pulls things towards each other. It's what keeps us on the ground and makes things fall down. You know how when you drop something, like a ball, it falls to the ground? That's because of gravity. Gravity is also what keeps the planets in our solar system orbiting around the sun. It's a really important force that affects everything in the universe!\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Assistant Function to Help Generaitng Python Programs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def ask_assitant(query):\n",
    "\n",
    "    MODEL = \"gpt-3.5-turbo\"\n",
    "    INSTRUCTIONS = \"\"\"\n",
    "    You are a personal assitant named Chatty helping the user to write python code. \\\n",
    "    Enclose code within ```python and ```. \\\n",
    "    Add [END] after python code.\"\n",
    "    \"\"\".strip()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        stop = \"[END]\"\n",
    "    )\n",
    "    \n",
    "    chat_response = response.choices[0].message.content\n",
    "    usage = response.usage.model_dump()\n",
    "\n",
    "    return chat_response, usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Code Generation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_response: Sure! Here's a Python code to calculate the area of a circle:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def calculate_area(radius):\n",
      "    area = math.pi * radius**2\n",
      "    return area\n",
      "\n",
      "radius = float(input(\"Enter the radius of the circle: \"))\n",
      "area = calculate_area(radius)\n",
      "print(\"The area of the circle is:\", area)\n",
      "```\n",
      "\n",
      "\n",
      "usage:{'completion_tokens': 72, 'prompt_tokens': 58, 'total_tokens': 130}\n"
     ]
    }
   ],
   "source": [
    "query = \"Write a Python code to calculate area of a circle?\"\n",
    "\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "chat_response, usage = ask_assitant(query)\n",
    "\n",
    "print(F\"chat_response: {chat_response}\\n\\nusage:{usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Python Code from the Chat Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "\n",
      "def calculate_area(radius):\n",
      "    area = math.pi * radius**2\n",
      "    return area\n",
      "\n",
      "radius = float(input(\"Enter the radius of the circle: \"))\n",
      "area = calculate_area(radius)\n",
      "print(\"The area of the circle is:\", area)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "python_code_pattern = r\"```python\\n([\\w\\s\\W]*)```\"\n",
    "python_code = re.findall(python_code_pattern, chat_response)[0]\n",
    "\n",
    "\n",
    "print(python_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Generated Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpython_code\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Type radius value in the input text box\u001b[39;00m\n",
      "File \u001b[1;32m<string>:7\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "exec(python_code) # Type radius value in the input text box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Assitant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def translation_assitant(query):\n",
    "\n",
    "    MODEL = \"gpt-3.5-turbo\"\n",
    "    INSTRUCTIONS = \"\"\"\n",
    "    Translate the given text into 1. French, 2. Spanish and 3. Japanese. Give answer in JSON using lnaguge names as keys and translated text as values\n",
    "    \"\"\".strip()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        stop = \"[END]\"\n",
    "    )\n",
    "    \n",
    "    chat_response = response.choices[0].message.content\n",
    "    usage = response.usage.model_dump()\n",
    "\n",
    "    return chat_response, usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"French\": \"Écrivez un code Python pour calculer l'aire d'un cercle.\",\n",
      "  \"Spanish\": \"Escribe un código Python para calcular el área de un círculo.\",\n",
      "  \"Japanese\": \"円の面積を計算するためのPythonコードを書いてください。\"\n",
      "}\n",
      "{'completion_tokens': 74, 'prompt_tokens': 58, 'total_tokens': 132}\n"
     ]
    }
   ],
   "source": [
    "text = \"How much is this milk gallon?\"\n",
    "\n",
    "chat_response, usage = translation_assitant(query)\n",
    "\n",
    "print(chat_response)\n",
    "print(usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy Completion `No longer supported`\n",
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow much is this milk gallon?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINSTRUCTION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m response\n",
      "File \u001b[1;32md:\\TeachingWorkspace\\GitHub\\BuildingAIApplicationsWithChatGPT-main\\.venv\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = openai_api_key\n",
    "MODEL = \"text-davinci-003\"\n",
    "\n",
    "INSTRUCTION = r'Translate the given text into 1. French, 2. Spanish and 3. Japanese:'\n",
    "text = \"How much is this milk gallon?\"\n",
    "\n",
    "prompt = F\"{INSTRUCTION}\\n\\n{text}\\n\\n\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=MODEL,\n",
    "  prompt=prompt,\n",
    "  temperature=0.3,\n",
    "  max_tokens=100,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = response['choices'][0]['text']\n",
    "usage = response['usage'].to_dict()\n",
    "\n",
    "print(F\"chat_response: {chat_response}\\n\\nusage:{usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "First Upload 2023-07-04 | Last update 2023-12-15 by Sumudu Tennakoon\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
